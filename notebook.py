# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mNYF69f-aW8y6uLj7frYmZ7eau0Obvnq

# Project System Recommendation : Movie
- **Nama:** Hafizha Aghnia Hasya
- **Email:** mc006d5x2114@student.devacademy.id
- **ID Dicoding:** MC006D5X2114

## Import Library

Pada tahap ini, dilakukan import berbagai library yang diperlukan untuk proses analisis, pemrosesan data, dan pembuatan sistem rekomendasi.
"""

from google.colab import files
import os
import json
import gc

import numpy as np
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import warnings
warnings.filterwarnings('ignore')

"""## Data Loading

Pada tahap ini, dataset didownload dari Kaggle kemudian dimuat menjadi csv menggunakan pandas.
"""

# download dataset dari kaggle

# Upload file kaggle.json tanpa menampilkan isinya
uploaded = files.upload()
# Buat folder ~/.kaggle jika belum ada
os.makedirs("/root/.kaggle", exist_ok=True)
# Pindahkan file ke folder konfigurasi Kaggle
with open("kaggle.json", "r") as f:
    kaggle_token = json.load(f)
# Simpan ke lokasi yang dibutuhkan Kaggle CLI
with open("/root/.kaggle/kaggle.json", "w") as f:
    json.dump(kaggle_token, f)
# Atur permission agar hanya dapat diakses oleh user
os.chmod("/root/.kaggle/kaggle.json", 0o600)

# Download kaggle dataset and unzip the file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d parasharmanas/movie-recommendation-system
!unzip movie-recommendation-system.zip

df_movie = pd.read_csv('movies.csv')
df_movie.head()

df_rating = pd.read_csv('ratings.csv')
df_rating.head()

"""## Data Cleaning

Tahap ini dilakukan untuk mendeteksi dan mengoreksi (atau menghapus) data yang tidak akurat, tidak lengkap, tidak relevan, duplikat, atau diformat dengan buruk dalam dataset.
Pada project ini, dilakukan drop fitur tidak relevan pada df_rating yaitu fitur timestamp, drop data film di bawah tahun 2005 yang tidak relevan lagi dengan user sekarang, dan menghapus data film yang tidak ada genre.
"""

# cek missing value
print(f'missing value pada df_movie : \n{df_movie.isnull().sum()} \n')
print(f'missing value pada df_rating : \n{df_rating.isnull().sum()}')

'''
fitur timestamp pada df_rating tidak relevan dengan sistem rekomendasi, maka akan didrop
'''
df_rating = df_rating.drop(columns='timestamp')
df_rating.head(1)

# cek data duplikat
print(f'data duplikat pada df_movie : \n{df_movie.duplicated().sum()}')
print(f'data duplikat pada df_rating : \n{df_rating.duplicated().sum()}')

'''
Film sebelum tahun 2005 didrop karena gaya dan kualitas filmnya mungkin sudah kurang relevan untuk preferensi penonton masa kini.
Selain itu, agar model rekomendasi nantinya fokus pada konten yang lebih kekinian.
'''

# Ekstrak tahun dari title
df_movie['year'] = df_movie['title'].str.extract(r'\((\d{4})\)')
df_movie['year'] = pd.to_numeric(df_movie['year'], errors='coerce')

# Filter film sebelum tahun 2005
movies_before_2005 = df_movie[df_movie['year'] < 2005]

# Tampilkan jumlah film sebelum tahun 2005
print(f"Jumlah film sebelum tahun 2005 : {len(movies_before_2005)}")

# Simpan movieId yang akan dihapus
movie_ids_to_remove = movies_before_2005['movieId'].tolist()

# Contoh: Lihat 5 film pertama yang akan dihapus
print(movies_before_2005[['movieId', 'title']].sample(5))

df_movie = df_movie[~df_movie['movieId'].isin(movie_ids_to_remove)]
df_rating = df_rating[~df_rating['movieId'].isin(movie_ids_to_remove)]

df_movie = df_movie.drop(columns='year')

print('banyak data setelah hapus film di bawah tahun 2000 :')
print('  df_movie :', df_movie.shape[0])
print('  df_rating :', df_rating.shape[0])

# memisahkan genre pada df_movie
df_movie['genres'] = df_movie['genres'].str.split('|')
df_movie.head(3)

# cek genre unik film
unique_genre = pd.Series([genre for genres_list in df_movie['genres'] for genre in genres_list]).unique()
unique_genre

# hapus data dengan '(no genres listed)' pada genres
df_movie = df_movie[~df_movie['genres'].apply(lambda x: '(no genres listed)' in x)]

# cek lagi genre unik film
unique_genre = pd.Series([genre for genres_list in df_movie['genres'] for genre in genres_list]).unique()
unique_genre

"""## Exploratory Data Analysis

Exploratory Data Analysis (EDA) adalah tahap eksplorasi data yang telah melalui proses pembersihan untuk memahami karakteristik, distribusi, pola, dan hubungan antar variabel dalam dataset. Pada tahap ini, berbagai teknik analisis statistik dan visualisasi data digunakan untuk mengidentifikasi tren, outlier, dan korelasi yang dapat memberikan wawasan lebih dalam terhadap data. EDA bertujuan untuk membantu menjawab pertanyaan analisis, menemukan pola tersembunyi, serta menjadi dasar dalam pengambilan keputusan sebelum melanjutkan ke tahap pemodelan atau analisis lebih lanjut.

### Statistics Descriptive

Tahapan ini bertujuan untuk memperoleh gambaran umum karakteristik data secara numerik maupun kategorik. Statistik deskriptif seperti mean, median, standard deviation, minimum, maksimum, dan kuartil digunakan untuk mengetahui sebaran dan nilai pusat dari fitur numerik.
"""

# cek informasi dataset
df_movie.info()
print()
df_rating.info()

df_movie.describe(include='all')

df_rating.describe()

"""### Univariate & Multivariate Analysis

Pada tahap ini, dilakukan visualisasi untuk menganalisis distribusi masing-masing fitur (univariate) dan korelasi antar fitur (multivariate) untuk memahami karakteristik data

#### df_movie
"""

# genre unik dari semua film
unique_genre = pd.Series([genre for genres_list in df_movie['genres'] for genre in genres_list]).unique()

print('banyak judul film :', df_movie['title'].nunique())
print('banyak genre film :', len(unique_genre))
print('genre film : \n',unique_genre)

# film per tahunnya
df_movie_temp = df_movie.copy()
df_movie_temp['year'] = df_movie_temp['title'].str.extract(r'\((\d{4})\)').astype(float)
df_movie_temp['year'].dropna().astype(int).value_counts().sort_index().plot(figsize=(10,5), color='salmon', title='Jumlah Film per Tahun')

# menampilkan bamyak film per genre
genre_count = pd.Series([genre for genres_list in df_movie['genres'] for genre in genres_list]).value_counts()

plt.figure(figsize=(10, 5))
genre_count.plot(kind='bar', color='salmon')
plt.title('Banyak Film per Genre')
plt.xlabel('Genre')
plt.ylabel('Banyak Film')
plt.xticks(rotation=45)
plt.show()

"""#### df_rating"""

# histogram rating

plt.figure(figsize=(6,4))
sns.histplot(df_rating['rating'], bins=[0.5 + 0.5*i for i in range(10)], kde=False, color='salmon', edgecolor='black')
plt.title('Distribusi Rating Film')
plt.xlabel('Frekuensi')
plt.ylabel('Jumlah')
plt.grid(True, linestyle='--', alpha=0.5)
plt.xticks([0.5 + 0.5*i for i in range(10)])
plt.show()

gc.collect()

"""## Model Development Content Based Filtering

Pada tahap ini, metode Content-Based Filtering digunakan untuk memberikan rekomendasi film berdasarkan kesamaan fitur dari film yang pernah dinikmati oleh pengguna. Metode ini memanfaatkan atribut konten film, seperti genre, untuk membangun profil item yang kemudian digunakan untuk mencari film-film dengan karakteristik serupa. Dengan demikian, sistem dapat merekomendasikan film yang relevan dan sesuai preferensi individu pengguna tanpa bergantung pada interaksi pengguna lain.
"""

df = df_movie.copy()
df.sample(5)

"""### Data Preparation

Pada tahap ini, data akan disiapkan untuk pengembangan/development model content based filtering dengan teknik TF-IDF Vectorizer dan Cosine Similarity.

#### TF-IDF Vectorizer

Pada tahap ini, akan diterapkan teknik TF-IDF Vectorizer untuk menemukan representasi fitur penting (genre) dari setiap film.
"""

# gabungkan genre jadi string per film
def preprocess_genres(genres):
    return [g.replace('-', '') for g in genres] # mengganti - menjadi _ seperti pada Sci-Fi menjadi Sci_Fi

df['genres_processed'] = df['genres'].apply(preprocess_genres)
df['genres_str'] = df['genres_processed'].apply(lambda x: ' '.join(x))

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer()

# Melakukan perhitungan idf pada data genre
tfidf.fit(df['genres_str'])

# Mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf.fit_transform(df['genres_str'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis masakan
# Baris diisi dengan nama movie

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=df.title
).sample(22, axis=1, replace=True).sample(10, axis=0)

gc.collect()

"""#### Cosine Similarity

Pada tahap ini, akan dihitung derajat kesamaan (similarity degree) antar restoran dengan teknik cosine similarity menggunakan fungsi cosine_similarity dari library sklearn.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama movie
cosine_sim_df = pd.DataFrame(cosine_sim, index=df['title'], columns=df['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap movie
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Mendapatkan Rekomendasi

Pada tahap ini, akan dibuat fungsi yang dapat menghasilkan sejumlah film yang akan direkomendasikan pada pengguna.

Di sini, dibuat fungsi resto_recommendations dengan beberapa parameter sebagai berikut:
- title : Nama fil, (index kemiripan dataframe).
- Similarity_data : Dataframe mengenai similarity yang telah didefinisikan sebelumnya.
- Items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah ‘title’ dan ‘genres’.
- k : Banyak rekomendasi yang ingin diberikan.
"""

def movie_recommendations(title, similarity_data=cosine_sim_df, items=df[['title', 'genres']], k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_movie agar nama movie yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

df_movie[df_movie.title.eq('Death Kiss (2018)')]

# Mendapatkan rekomendasi movie yang mirip dengan Death Kiss (2018)
movie_recommendations('Death Kiss (2018)')

"""## Model Development dengan Collaborative Filtering

Pada tahap ini, dilakukan pengembangan model sistem rekomendasi menggunakan pendekatan Collaborative Filtering. Metode ini memanfaatkan interaksi historis antara pengguna dan item (dalam hal ini, film) untuk memprediksi preferensi pengguna terhadap item yang belum pernah mereka konsumsi. Model dikembangkan berdasarkan data rating yang diberikan oleh pengguna terhadap sejumlah film, dengan tujuan memberikan rekomendasi yang dipersonalisasi dan relevan.
"""

df2 = df_rating.copy()
df2.sample(5)

"""### Data Preparation

Pada tahap ini, data akan disiapkan untuk pengembangan/development collaborative filtering dengan mendefinisikan beberapa hal seperti banyak user/film, dan juga membagi data menjadi data train 80% dan data validasi 20%
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df2['userId'].unique().tolist()

# Mengubah movieID menjadi list tanpa nilai yang sama
movie_ids = df2['movieId'].unique().tolist()

# Mendapatkan jumlah user
num_users = len(user_ids)
print(num_users)

# Mendapatkan jumlah movie
num_movie = len(movie_ids)
print(num_movie)

# Nilai minimum rating
min_rating = min(df2['rating'])

# Nilai maksimal rating
max_rating = max(df2['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""#### Membagi Data untuk Training dan Validasi

Setelah data diacak, dilakukan pembagian data training dan validasi dengan rasio 80:20
"""

# Mengacak dataset
df2 = df2.sample(frac=1, random_state=42)
df2

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = df2[['userId', 'movieId']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df2['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df2.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""### Proses Training

Pada tahap ini, model menghitung skor kecocokan antara pengguna dan film dengan teknik embedding. Pertama, kita melakukan proses embedding terhadap data user dan film. Selanjutnya, lakukan operasi perkalian dot product antara embedding user dan film. Selain itu, kita juga dapat menambahkan bias untuk setiap user dan film. Skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid.

Di sini, kita membuat class RecommenderNet dengan keras Model class.
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 128,
    epochs = 5,
    validation_data = (x_val, y_val)
)

"""### Visualisasi Metrik

Tahap ini dilakukan untuk melihat visualisasi proses training, mari kita plot metrik evaluasi dengan matplotlib
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Mendapatkan Rekomendasi

Untuk mendapatkan rekomendasi film, pertama kita ambil sampel user secara acak dan definisikan variabel movie_not_watched yang merupakan daftar film yang belum pernah ditonton oleh pengguna.

Sebelumnya, pengguna telah memberi rating pada beberapa film yang telah mereka tonton. Kita menggunakan rating ini untuk membuat rekomendasi film yang mungkin cocok untuk pengguna. Nah, film yang akan direkomendasikan tentulah film yang belum pernah ditonton oleh pengguna. Oleh karena itu, kita perlu membuat variabel movie_not_watched sebagai daftar film untuk direkomendasikan pada pengguna.

Variabel movie_not_watched diperoleh dengan menggunakan operator bitwise (~) pada variabel movie_watched_by_user.
"""

movie_df = df.copy()
rating_df = df2.copy()

# Mengambil sample user
user_id = df2.userId.sample(1).iloc[0]
movie_watched_by_user = df2[df2.userId == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
movie_not_watched = movie_df[~movie_df['movieId'].isin(movie_watched_by_user.movieId.values)]['movieId']

user_movie_array = np.hstack((
    np.array([[user_id]] * len(movie_not_watched)),
    np.array(movie_not_watched).reshape(-1, 1)
))

ratings = model.predict(user_movie_array).flatten()

# Ambil indeks dengan prediksi rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Karena kita pakai ID asli, ambil langsung dari movie_not_watched
recommended_movie_ids = [movie_not_watched.iloc[x] for x in top_ratings_indices]

print('Showing recommendations for user:', user_id)
print('===' * 9)
print('Film with high ratings from user')
print('----' * 8)

# Ambil 5 film dengan rating tertinggi dari user tersebut
top_movie_user = (
    movie_watched_by_user.sort_values(
        by='rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

# Tampilkan film yang sudah pernah ditonton dan disukai user
movie_df_rows = movie_df[movie_df['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres)

print('----' * 8)
print('Top 10 film recommendation')
print('----' * 8)

# Tampilkan film hasil rekomendasi model
recommended_movie = movie_df[movie_df['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)